{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead72164-3825-4469-972c-15bc64d87dc8",
   "metadata": {},
   "source": [
    "# Statistical Backtesting Time Series for Freqtrade\n",
    "This notebook helps you define a proper timerange for training, validating and testing your strategy.\n",
    "\n",
    "Classical way of backtesting:\n",
    "- split data into train / validate / test 0.6 0.2 0.2\n",
    "- train dataset: train & fit the model, adjust params\n",
    "- validate dataset: eval performance, hyperparam, select best model\n",
    "- test dataset: evaluate performance (final evaluation, unseen data, estimates real world scenario)\n",
    "- never go back on the data after the test dataset (as you're overfitting to the test)\n",
    "- ... unless you are prepared for another test dataset / dry run\n",
    "\n",
    "A better way:\n",
    "- define your strategy\n",
    "- run the Unanchored Walk Forward Optimization to get the folds\n",
    "- train your dataset on the first training fold\n",
    "- test it on the first test fold\n",
    "- repeat until the last fold\n",
    "- how did your strategy behave after experiencing all those market conditions?\n",
    "\n",
    "I recommend using the Walk Forward Optimization which, if correctly implemented, is one of the best ways to avoid overfitting a trading strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52aead-a809-461a-969f-68dbba629139",
   "metadata": {},
   "source": [
    "## Imports & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3beec2e0-891c-4463-b95c-0ce4b4e8352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943e9dde-c03b-498d-b552-b200f64da6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timerange: 20210101-20230821\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/freqtrade\")\n",
    "token = pd.read_feather('user_data/data/binance/futures/BTC_USDT_USDT-1d-futures.feather')\n",
    "start_date = token['date'].min().strftime('%Y%m%d')\n",
    "end_date = token['date'].max().strftime('%Y%m%d')\n",
    "\n",
    "print(f\"Timerange: {start_date}-{end_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e6c561-276d-4af5-8ad4-a4fac4e40810",
   "metadata": {},
   "source": [
    "## Preferred methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697de3af-cd37-4a66-8c17-764f6c1f3fa3",
   "metadata": {},
   "source": [
    "### Unanchored Walk Forward Optimization\n",
    "Here we have a rolling window of training/test data and we continuously adapt to the market.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc383b5-1efc-46f8-9e42-e01692b4f67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "\tTrain:\t20210101-20210711\t(192, 2)\n",
      "\tTest:\t20210712-20210818\t(192, 2)\n",
      "Fold 2:\n",
      "\tTrain:\t20210712-20220119\t(192, 2)\n",
      "\tTest:\t20220120-20220226\t(192, 2)\n",
      "Fold 3:\n",
      "\tTrain:\t20220120-20220730\t(192, 2)\n",
      "\tTest:\t20220731-20220906\t(192, 2)\n",
      "Fold 4:\n",
      "\tTrain:\t20220731-20230207\t(192, 2)\n",
      "\tTest:\t20230208-20230317\t(192, 2)\n",
      "Fold 5:\n",
      "\tTrain:\t20230208-20230714\t(157, 2)\n",
      "\tTest:\t20230715-20230821\t(157, 2)\n",
      "\n",
      "\n",
      "fold1_train=\"20210101-20210711\"\n",
      "fold1_test=\"20210712-20210818\"\n",
      "fold2_train=\"20210712-20220119\"\n",
      "fold2_test=\"20220120-20220226\"\n",
      "fold3_train=\"20220120-20220730\"\n",
      "fold3_test=\"20220731-20220906\"\n",
      "fold4_train=\"20220731-20230207\"\n",
      "fold4_test=\"20230208-20230317\"\n",
      "fold5_train=\"20230208-20230714\"\n",
      "fold5_test=\"20230715-20230821\"\n"
     ]
    }
   ],
   "source": [
    "df = token[['date', 'close']]\n",
    "\n",
    "# Define the number of splits and the test data compared to the training data (0.2 = 80% training 20% testing)\n",
    "n_splits = 5\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Calculate the step size for each fold\n",
    "step_size = len(df) // n_splits\n",
    "\n",
    "# Create an empty list to store the folds\n",
    "folds = []\n",
    "\n",
    "# Split the data into folds with unanchored walk-forward\n",
    "for i in range(n_splits):\n",
    "    start_idx = i * step_size\n",
    "    test_days = int((step_size + 1) * test_ratio)\n",
    "    end_idx = start_idx + step_size + test_days\n",
    "    \n",
    "    \n",
    "    # Ensure the end index is within the data range\n",
    "    if end_idx > len(df):\n",
    "        end_idx = len(df)\n",
    "    \n",
    "    train_fold = df.iloc[start_idx:end_idx - test_days]\n",
    "    test_fold = df.iloc[end_idx - test_days:end_idx]\n",
    "    folds.append((train_fold, test_fold))\n",
    "\n",
    "# Display the data and time ranges in each fold\n",
    "for i, (train_fold, test_fold) in enumerate(folds):\n",
    "    train_start_date = pd.to_datetime(train_fold['date'].min()).strftime('%Y%m%d')\n",
    "    train_end_date = pd.to_datetime(train_fold['date'].max()).strftime('%Y%m%d')\n",
    "    test_start_date = pd.to_datetime(test_fold['date'].min()).strftime('%Y%m%d')\n",
    "    test_end_date = pd.to_datetime(test_fold['date'].max()).strftime('%Y%m%d')\n",
    "    \n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"\\tTrain:\\t{train_start_date}-{train_end_date}\\t{train_fold.shape}\")\n",
    "    print(f\"\\tTest:\\t{test_start_date}-{test_end_date}\\t{train_fold.shape}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display the fold ranges and their values after the loop\n",
    "for i, (train_fold, test_fold) in enumerate(folds):\n",
    "    fold_train_range = f\"fold{i+1}_train=\\\"{pd.to_datetime(train_fold['date'].min()).strftime('%Y%m%d')}-{pd.to_datetime(train_fold['date'].max()).strftime('%Y%m%d')}\\\"\"\n",
    "    fold_test_range = f\"fold{i+1}_test=\\\"{pd.to_datetime(test_fold['date'].min()).strftime('%Y%m%d')}-{pd.to_datetime(test_fold['date'].max()).strftime('%Y%m%d')}\\\"\"\n",
    "    \n",
    "    print(f\"{fold_train_range}\")\n",
    "    print(f\"{fold_test_range}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659af6d2-855d-4475-9aab-416285285084",
   "metadata": {},
   "source": [
    "### Splitting the training dataset via kfold into separate timeranges with test sets\n",
    "Here we split the data into 5 folds.\n",
    "There's a bug in which the test set of the last fold might be too small but you can use the last fold for a final test.\n",
    "The disadvantaged with this method compared to a walk forward approach is that we don't adapt to the new market regime and our old data might have the same impact as the new data has. Think Moving Average vs Exponential Moving Average.\n",
    "Read more here: https://medium.com/eatpredlove/time-series-cross-validation-a-walk-forward-approach-in-python-8534dd1db51a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1d7566-86af-49a7-819a-bafc976e86a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 192\n",
      "\n",
      "Fold 1:\n",
      "\tTrain:\t20210101-20210711\t(192, 2)\n",
      "\tTest:\t20210712-20220119\t(192, 2)\n",
      "Fold 2:\n",
      "\tTrain:\t20210712-20220119\t(192, 2)\n",
      "\tTest:\t20220120-20220730\t(192, 2)\n",
      "Fold 3:\n",
      "\tTrain:\t20220120-20220730\t(192, 2)\n",
      "\tTest:\t20220731-20230207\t(192, 2)\n",
      "Fold 4:\n",
      "\tTrain:\t20220731-20230207\t(192, 2)\n",
      "\tTest:\t20230208-20230818\t(192, 2)\n",
      "Fold 5:\n",
      "\tTrain:\t20230208-20230818\t(192, 2)\n",
      "\tTest:\t20230819-20230821\t(3, 2)\n",
      "\n",
      "\n",
      "fold1_train=\"20210101-20210711\"\n",
      "fold1_test=\"20210712-20220119\"\n",
      "fold2_train=\"20210712-20220119\"\n",
      "fold2_test=\"20220120-20220730\"\n",
      "fold3_train=\"20220120-20220730\"\n",
      "fold3_test=\"20220731-20230207\"\n",
      "fold4_train=\"20220731-20230207\"\n",
      "fold4_test=\"20230208-20230818\"\n",
      "fold5_train=\"20230208-20230818\"\n",
      "fold5_test=\"20230819-20230821\"\n"
     ]
    }
   ],
   "source": [
    "df = token[['date', 'close']]\n",
    "\n",
    "# Define the number of splits\n",
    "n_splits = 5\n",
    "\n",
    "# Calculate the step size for each fold\n",
    "step_size = len(df) // n_splits\n",
    "\n",
    "# Create an empty list to store the folds\n",
    "folds = []\n",
    "\n",
    "print(f\"Step size: {step_size}\\n\")\n",
    "\n",
    "# Split the data into folds\n",
    "for i in range(n_splits):\n",
    "    start_idx = i * step_size\n",
    "    end_idx = start_idx + step_size\n",
    "\n",
    "    # Split the data into train and test based on indices\n",
    "    train_fold = df.iloc[start_idx:end_idx]\n",
    "    test_fold = df.iloc[end_idx:end_idx + step_size] if i < n_splits - 1 else df.iloc[end_idx:]\n",
    "    \n",
    "    folds.append((train_fold, test_fold))\n",
    "\n",
    "# Display the data and time ranges in each fold\n",
    "for i, (train_fold, test_fold) in enumerate(folds):\n",
    "    train_start_date = pd.to_datetime(train_fold['date'].min()).strftime('%Y%m%d')\n",
    "    train_end_date = pd.to_datetime(train_fold['date'].max()).strftime('%Y%m%d')\n",
    "    test_start_date = pd.to_datetime(test_fold['date'].min()).strftime('%Y%m%d')\n",
    "    test_end_date = pd.to_datetime(test_fold['date'].max()).strftime('%Y%m%d')\n",
    "        \n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"\\tTrain:\\t{train_start_date}-{train_end_date}\\t{train_fold.shape}\")\n",
    "    print(f\"\\tTest:\\t{test_start_date}-{test_end_date}\\t{test_fold.shape}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display the fold ranges and their values after the loop\n",
    "for i, (train_fold, test_fold) in enumerate(folds):\n",
    "    fold_train_range = f\"fold{i+1}_train=\\\"{pd.to_datetime(train_fold['date'].min()).strftime('%Y%m%d')}-{pd.to_datetime(train_fold['date'].max()).strftime('%Y%m%d')}\\\"\"\n",
    "    fold_test_range = f\"fold{i+1}_test=\\\"{pd.to_datetime(test_fold['date'].min()).strftime('%Y%m%d')}-{pd.to_datetime(test_fold['date'].max()).strftime('%Y%m%d')}\\\"\"\n",
    "    \n",
    "    print(f\"{fold_train_range}\")\n",
    "    print(f\"{fold_test_range}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d461e347-cfc0-4c92-8de0-9d97cb450538",
   "metadata": {},
   "source": [
    "### Anchored Walk Forward Optimization\n",
    "Here we split the data starting from the same start date and see how our strategy evolves, how the parameters are modified up to fold 5.\n",
    "Each fold has a training period and a test period. The advantage of anchored is that we have bigger training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a430761-5900-4504-b631-c23631299f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "\tTrain:\t20210101-20210612\t(163, 2)\n",
      "\tTest:\t20210613-20211119\t(163, 2)\n",
      "Fold 2:\n",
      "\tTrain:\t20210101-20211119\t(323, 2)\n",
      "\tTest:\t20211120-20220428\t(323, 2)\n",
      "Fold 3:\n",
      "\tTrain:\t20210101-20220428\t(483, 2)\n",
      "\tTest:\t20220429-20221005\t(483, 2)\n",
      "Fold 4:\n",
      "\tTrain:\t20210101-20221005\t(643, 2)\n",
      "\tTest:\t20221006-20230314\t(643, 2)\n",
      "Fold 5:\n",
      "\tTrain:\t20210101-20230314\t(803, 2)\n",
      "\tTest:\t20230315-20230821\t(803, 2)\n",
      "\n",
      "\n",
      "fold1_train=\"20210101-20210612\"\n",
      "fold1_test=\"20210613-20211119\"\n",
      "fold2_train=\"20210101-20211119\"\n",
      "fold2_test=\"20211120-20220428\"\n",
      "fold3_train=\"20210101-20220428\"\n",
      "fold3_test=\"20220429-20221005\"\n",
      "fold4_train=\"20210101-20221005\"\n",
      "fold4_test=\"20221006-20230314\"\n",
      "fold5_train=\"20210101-20230314\"\n",
      "fold5_test=\"20230315-20230821\"\n"
     ]
    }
   ],
   "source": [
    "df = token[['date', 'close']]\n",
    "\n",
    "# Define the number of splits\n",
    "n_splits = 5\n",
    "\n",
    "# Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Create an empty list to store the folds\n",
    "folds = []\n",
    "\n",
    "# Split the data using TimeSeriesSplit\n",
    "for train_idx, test_idx in tscv.split(df):\n",
    "    train_fold = df.iloc[train_idx]\n",
    "    test_fold = df.iloc[test_idx]\n",
    "    folds.append((train_fold, test_fold))\n",
    "\n",
    "# Display the data and time ranges in each fold\n",
    "for i, (train_fold, test_fold) in enumerate(folds):\n",
    "    train_start_date = pd.to_datetime(train_fold['date'].min()).strftime('%Y%m%d')\n",
    "    train_end_date = pd.to_datetime(train_fold['date'].max()).strftime('%Y%m%d')\n",
    "    test_start_date = pd.to_datetime(test_fold['date'].min()).strftime('%Y%m%d')\n",
    "    test_end_date = pd.to_datetime(test_fold['date'].max()).strftime('%Y%m%d')\n",
    "    \n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"\\tTrain:\\t{train_start_date}-{train_end_date}\\t{train_fold.shape}\")\n",
    "    print(f\"\\tTest:\\t{test_start_date}-{test_end_date}\\t{train_fold.shape}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display the fold ranges and their values after the loop\n",
    "for i, (train_fold, test_fold) in enumerate(folds):\n",
    "    fold_train_range = f\"fold{i+1}_train=\\\"{pd.to_datetime(train_fold['date'].min()).strftime('%Y%m%d')}-{pd.to_datetime(train_fold['date'].max()).strftime('%Y%m%d')}\\\"\"\n",
    "    fold_test_range = f\"fold{i+1}_test=\\\"{pd.to_datetime(test_fold['date'].min()).strftime('%Y%m%d')}-{pd.to_datetime(test_fold['date'].max()).strftime('%Y%m%d')}\\\"\"\n",
    "    \n",
    "    print(f\"{fold_train_range}\")\n",
    "    print(f\"{fold_test_range}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d372430-371b-493c-9310-ffe69d9cf9bb",
   "metadata": {},
   "source": [
    "### Anchored Walk Forward Optimization v2\n",
    "Here we split the data starting from the same start date and see how our strategy evolves, how the parameters are modified up to fold 5.\n",
    "Each fold has a training period and a test period. The difference in v2 is that we have a longer training period in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e73a873-3bc0-481d-b0f2-8ec728947697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "\tTrain:\t20210101-20211231\t(365, 2)\n",
      "\tTest:\t20220101-20220301\t(365, 2)\n",
      "Fold 2:\n",
      "\tTrain:\t20210101-20220629\t(545, 2)\n",
      "\tTest:\t20220630-20220828\t(545, 2)\n",
      "Fold 3:\n",
      "\tTrain:\t20210101-20221226\t(725, 2)\n",
      "\tTest:\t20221227-20230224\t(725, 2)\n",
      "Fold 4:\n",
      "\tTrain:\t20210101-20230624\t(905, 2)\n",
      "\tTest:\t20230625-20230821\t(905, 2)\n",
      "\n",
      "\n",
      "fold1_train=\"20210101-20211231\"\n",
      "fold1_test=\"20220101-20220301\"\n",
      "fold2_train=\"20210101-20220629\"\n",
      "fold2_test=\"20220630-20220828\"\n",
      "fold3_train=\"20210101-20221226\"\n",
      "fold3_test=\"20221227-20230224\"\n",
      "fold4_train=\"20210101-20230624\"\n",
      "fold4_test=\"20230625-20230821\"\n"
     ]
    }
   ],
   "source": [
    "df = token[['date', 'close']]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# credits: https://medium.com/eatpredlove/time-series-cross-validation-a-walk-forward-approach-in-python-8534dd1db51a\n",
    "class expanding_window(object):\n",
    "    '''\t\n",
    "    Parameters \n",
    "    ----------\n",
    "    \n",
    "    Note that if you define a horizon that is too far, then subsequently the split will ignore horizon length \n",
    "    such that there is validation data left. This similar to Prof Rob hyndman's TsCv \n",
    "    \n",
    "    \n",
    "    initial: int\n",
    "        initial train length \n",
    "    horizon: int \n",
    "        forecast horizon (forecast length). Default = 1\n",
    "    period: int \n",
    "        length of train data to add each iteration \n",
    "    '''\n",
    "    \n",
    "\n",
    "    def __init__(self,initial= 1,horizon = 1,period = 1):\n",
    "        self.initial = initial\n",
    "        self.horizon = horizon \n",
    "        self.period = period \n",
    "\n",
    "\n",
    "    def split(self,data):\n",
    "        '''\n",
    "        Parameters \n",
    "        ----------\n",
    "        \n",
    "        Data: Training data \n",
    "        \n",
    "        Returns \n",
    "        -------\n",
    "        train_index ,test_index: \n",
    "            index for train and valid set similar to sklearn model selection\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.counter = 0 # for us to iterate and track later \n",
    "\n",
    "\n",
    "        data_length = data.shape[0] # rows \n",
    "        data_index = list(np.arange(data_length))\n",
    "         \n",
    "        output_train = []\n",
    "        output_test = []\n",
    "        # append initial \n",
    "        output_train.append(list(np.arange(self.initial)))\n",
    "        progress = [x for x in data_index if x not in list(np.arange(self.initial)) ] # indexes left to append to train \n",
    "        output_test.append([x for x in data_index if x not in output_train[self.counter]][:self.horizon] )\n",
    "        # clip initial indexes from progress since that is what we are left \n",
    "         \n",
    "        while len(progress) != 0:\n",
    "            temp = progress[:self.period]\n",
    "            to_add = output_train[self.counter] + temp\n",
    "            # update the train index \n",
    "            output_train.append(to_add)\n",
    "            # increment counter \n",
    "            self.counter +=1 \n",
    "            # then we update the test index \n",
    "            \n",
    "            to_add_test = [x for x in data_index if x not in output_train[self.counter] ][:self.horizon]\n",
    "            output_test.append(to_add_test)\n",
    "\n",
    "            # update progress \n",
    "            progress = [x for x in data_index if x not in output_train[self.counter]]\t\n",
    "            \n",
    "        # clip the last element of output_train and output_test\n",
    "        output_train = output_train[:-1]\n",
    "        output_test = output_test[:-1]\n",
    "        \n",
    "        # mimic sklearn output \n",
    "        index_output = [(train,test) for train,test in zip(output_train,output_test)]\n",
    "        \n",
    "        return index_output\n",
    "\n",
    "\n",
    "# We train on 1 year worth of data, then we have a test data length of 2 months and we add 6 months to each iteration (121 days)\n",
    "tscv = expanding_window(initial = 365, horizon = 2*30,period = 6*30)\n",
    "\n",
    "# Create an empty list to store the folds\n",
    "folds = []\n",
    "\n",
    "# Split the data using TimeSeriesSplit\n",
    "for train_idx, test_idx in tscv.split(df):\n",
    "    train_fold = df.iloc[train_idx]\n",
    "    test_fold = df.iloc[test_idx]\n",
    "    folds.append((train_fold, test_fold))\n",
    "\n",
    "# Display the data and time ranges in each fold\n",
    "for i, (train_fold, test_fold) in enumerate(folds):\n",
    "    train_start_date = pd.to_datetime(train_fold['date'].min()).strftime('%Y%m%d')\n",
    "    train_end_date = pd.to_datetime(train_fold['date'].max()).strftime('%Y%m%d')\n",
    "    test_start_date = pd.to_datetime(test_fold['date'].min()).strftime('%Y%m%d')\n",
    "    test_end_date = pd.to_datetime(test_fold['date'].max()).strftime('%Y%m%d')\n",
    "    \n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"\\tTrain:\\t{train_start_date}-{train_end_date}\\t{train_fold.shape}\")\n",
    "    print(f\"\\tTest:\\t{test_start_date}-{test_end_date}\\t{train_fold.shape}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display the fold ranges and their values after the loop\n",
    "for i, (train_fold, test_fold) in enumerate(folds):\n",
    "    fold_train_range = f\"fold{i+1}_train=\\\"{pd.to_datetime(train_fold['date'].min()).strftime('%Y%m%d')}-{pd.to_datetime(train_fold['date'].max()).strftime('%Y%m%d')}\\\"\"\n",
    "    fold_test_range = f\"fold{i+1}_test=\\\"{pd.to_datetime(test_fold['date'].min()).strftime('%Y%m%d')}-{pd.to_datetime(test_fold['date'].max()).strftime('%Y%m%d')}\\\"\"\n",
    "    \n",
    "    print(f\"{fold_train_range}\")\n",
    "    print(f\"{fold_test_range}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340d89a-a521-4d6e-8182-75662823455b",
   "metadata": {},
   "source": [
    "## Not used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5515e-6820-4f07-bfdb-2b2fcb6962f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Splitting the data into a separate training, validation and test dataset\n",
    "Normally we would split the data into a 80-20-20 split for training, validation and testing. \n",
    "With TimeSeries this would just hit different types of markets (e.g. training on a downtrend and testing on an uptrend).\n",
    "The results might be too optimistic (or pessimistic) and not statistically sound. \n",
    "So we're skipping it in favor of a walk forward optimization or a kfold split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52688f2-b07b-47ac-a2bf-1dd1d6bc5132",
   "metadata": {},
   "source": [
    "### Splitting the data using train_test_split() 60-40\n",
    "\n",
    "The issue with train_test_split() is that you might train your model in a specific market situation like a downtrend and test it right when the market changes, maybe to an uptrend. On a long biased strategy this would show more profit.\n",
    "\n",
    "Try using the other methods instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320ba56d-baaf-4132-84e1-1ed7ad7ad985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t\t20210101-20230209\n",
      "Test:\t\t20230210-20230821\n",
      "\n",
      "Training length:\t770\n",
      "Testing length:\t\t193\n"
     ]
    }
   ],
   "source": [
    "df = token[['date', 'close']]\n",
    "\n",
    "# Splitting the data into training (80%) and test (20%)\n",
    "train, test = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Get the date ranges for training and validation sets\n",
    "train_start_date = train['date'].iloc[0].strftime('%Y%m%d')\n",
    "train_end_date = train['date'].iloc[-1].strftime('%Y%m%d')\n",
    "test_start_date = test['date'].iloc[0].strftime('%Y%m%d')\n",
    "test_end_date = test['date'].iloc[-1].strftime('%Y%m%d')\n",
    "\n",
    "# Display the date ranges\n",
    "print(f\"Train:\\t\\t{train_start_date}-{train_end_date}\")\n",
    "print(f\"Test:\\t\\t{test_start_date}-{test_end_date}\")\n",
    "\n",
    "print(f\"\\nTraining length:\\t{len(train)}\")\n",
    "print(f\"Testing length:\\t\\t{len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c4fba4-4c74-4ff6-837b-a8ad8d5c624d",
   "metadata": {},
   "source": [
    "### Splitting the data using train_test_split() 60-20-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f6ed6e-ea97-46f2-a882-9a33694aa83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t\t20210101-20220731\n",
      "Validate:\t20220801-20230209\n",
      "Test:\t\t20230210-20230821\n",
      "\n",
      "Training length:\t577\n",
      "Validation length:\t193\n",
      "Testing length:\t\t193\n"
     ]
    }
   ],
   "source": [
    "df = token[['date', 'close']]\n",
    "\n",
    "# Splitting the data into training (60%) and temp (40%)\n",
    "train, temp = train_test_split(df, test_size=0.4, shuffle=False)\n",
    "\n",
    "# Splitting the temp data into validation (50%) and test (50%) to achieve an overall split of 60-20-20\n",
    "val, test = train_test_split(temp, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Get the date ranges for training and validation sets\n",
    "train_start_date = train['date'].iloc[0].strftime('%Y%m%d')\n",
    "train_end_date = train['date'].iloc[-1].strftime('%Y%m%d')\n",
    "val_start_date = val['date'].iloc[0].strftime('%Y%m%d')\n",
    "val_end_date = val['date'].iloc[-1].strftime('%Y%m%d')\n",
    "test_start_date = test['date'].iloc[0].strftime('%Y%m%d')\n",
    "test_end_date = test['date'].iloc[-1].strftime('%Y%m%d')\n",
    "\n",
    "# Display the date ranges\n",
    "print(f\"Train:\\t\\t{train_start_date}-{train_end_date}\")\n",
    "print(f\"Validate:\\t{val_start_date}-{val_end_date}\")\n",
    "print(f\"Test:\\t\\t{test_start_date}-{test_end_date}\")\n",
    "\n",
    "print(f\"\\nTraining length:\\t{len(train)}\")\n",
    "print(f\"Validation length:\\t{len(val)}\")\n",
    "print(f\"Testing length:\\t\\t{len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800241ae-1a36-4553-9020-bb688b7d1e76",
   "metadata": {},
   "source": [
    "### Splitting the data using KFold\n",
    "This approach is problematic because it overfits by testing on data that we've learned from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a74101b-37db-456f-a242-3ad5a5a55662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t\t20210713-20230821\t\tshape:\t(770, 2)\n",
      "Test:\t\t20210101-20210712\t\tshape:\t(193, 2)\n",
      "------\n",
      "Train:\t\t20210101-20230821\t\tshape:\t(770, 2)\n",
      "Test:\t\t20210713-20220121\t\tshape:\t(193, 2)\n",
      "------\n",
      "Train:\t\t20210101-20230821\t\tshape:\t(770, 2)\n",
      "Test:\t\t20220122-20220802\t\tshape:\t(193, 2)\n",
      "------\n",
      "Train:\t\t20210101-20230821\t\tshape:\t(771, 2)\n",
      "Test:\t\t20220803-20230210\t\tshape:\t(192, 2)\n",
      "------\n",
      "Train:\t\t20210101-20230210\t\tshape:\t(771, 2)\n",
      "Test:\t\t20230211-20230821\t\tshape:\t(192, 2)\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "df = token[['date', 'close']]\n",
    "\n",
    "# Initialize KFold with the desired number of splits\n",
    "kfold = KFold(n_splits=5, shuffle=False)  # Let's use 5 splits as an example\n",
    "\n",
    "# Define date format for printing\n",
    "date_format = \"%Y%m%d\"\n",
    "\n",
    "for train_idx, test_idx in kfold.split(df):\n",
    "    train_data, test_data = df.iloc[train_idx], df.iloc[test_idx]    \n",
    "    \n",
    "    # Get date ranges\n",
    "    train_start_date = train_data['date'].min().strftime(date_format)\n",
    "    train_end_date = train_data['date'].max().strftime(date_format)\n",
    "    test_start_date = test_data['date'].min().strftime(date_format)\n",
    "    test_end_date = test_data['date'].max().strftime(date_format)\n",
    "    \n",
    "    # Print the train and test data ranges\n",
    "    print(f\"Train:\\t\\t{train_start_date}-{train_end_date}\\t\\tshape:\\t{train_data.shape}\")\n",
    "    print(f\"Test:\\t\\t{test_start_date}-{test_end_date}\\t\\tshape:\\t{test_data.shape}\")\n",
    "    print(\"------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb1b4b-7f45-4a8f-a8fe-d973e7fbf398",
   "metadata": {},
   "source": [
    "### Found  GroupTimeSeriesSplit on Kaggle which might provide another option of splitting the timeframes\n",
    "https://www.kaggle.com/code/jorijnsmit/found-the-holy-grail-grouptimeseriessplit/comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff931ff3-bfbd-41be-8e66-4634f470198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = token[['date', 'close']]\n",
    "\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class GroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_size : int, default=None\n",
    "        Maximum size for a single training set.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n",
    "    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n",
    "                           'b', 'b', 'b', 'b', 'b',\\\n",
    "                           'c', 'c', 'c', 'c',\\\n",
    "                           'd', 'd', 'd'])\n",
    "    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n",
    "    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n",
    "    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n",
    "                  \"TEST GROUP:\", groups[test_idx])\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n",
    "    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n",
    "    TEST GROUP: ['c' 'c' 'c' 'c']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n",
    "    TEST: [15, 16, 17]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n",
    "    TEST GROUP: ['d' 'd' 'd']\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_size=None\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_size = max_train_size\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "        group_test_size = n_groups // n_folds\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "            for train_group_idx in unique_groups[:group_test_start]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    "            if self.max_train_size and self.max_train_size < train_end:\n",
    "                train_array = train_array[train_end -\n",
    "                                          self.max_train_size:train_end]\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
